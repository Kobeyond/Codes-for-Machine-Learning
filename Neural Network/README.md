# Neural Network


## Scalar BP

The `scalar` back propagation formulars are below:

<img width='250' height='274' src="https://github.com/Kobeyond/Codes-for-Machine-Learning/blob/master/Neural%20Network/data/scalar_bp.png"/>

In addition, the complete process of deduction is:

<img width='700' height='761' src="https://github.com/Kobeyond/Codes-for-Machine-Learning/blob/master/Neural%20Network/data/scalar.png"/>

## Vectorized BP

Actually, when we train the network, it's more efficient to get the gradient vectors of weight and bias directly, by using `vectorized BP formulars`, rather than use 'for' loop to get it one by one. The vectorized formulars are below:

<img width='300' height='216' src="https://github.com/Kobeyond/Codes-for-Machine-Learning/blob/master/Neural%20Network/data/vectorized_bp.png"/>


Similarly, the complete process of deduction is:

<img width='950' height='455' src="https://github.com/Kobeyond/Codes-for-Machine-Learning/blob/master/Neural%20Network/data/vectorized.png"/>

